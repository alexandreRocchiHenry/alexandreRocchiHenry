<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8">
  <title>Projets - Alexandre Rocchi</title>
  <link rel="stylesheet" href="assets/css/projects.css">
</head>
<body>
    <div id="header-placeholder"></div>
    <script>
      fetch('assets/html/header.html')
        .then(response => response.text())
        .then(data => {
          document.getElementById('header-placeholder').innerHTML = data;
        });
    </script>
  <main>
    <!-- ------------------------------------------------------------------------
         PROJET 1 : Airbus - Détection de Changements Environnementaux
    ------------------------------------------------------------------------- -->
    <section class="projects" id="airbusProject">
      <h2>Projet Airbus - Détection de Changements Environnementaux</h2>
      <p class="ss-titre">Projet académique - Computer Vision &amp; Telecom Paris</p>
      <article>
        <h3>Description du projet</h3>
        <p>
          Dans ce projet mené en groupe, nous avons cherché à détecter, sans supervision, 
          des changements inattendus dans des séries d'images satellitaires prises à 
          différentes dates. L’objectif final était de classer par ordre d’importance 
          les anomalies d’ordre environnemental, afin de concentrer notre analyse sur 
          les zones présentant des évolutions majeures dans l’occupation du sol.
        </p>

        <h3>Approche</h3>
        <p>
          Nous nous sommes appuyés sur des modèles de fondation pour extraire 
          des représentations (embeddings) locales, puis nous avons comparé ces 
          vecteurs à l’aide de mesures de similarité comme la distance cosinus 
          et euclidienne. Le pipeline comprend également plusieurs étapes 
          (prétraitement, clustering et visualisation) afin d’obtenir une 
          analyse plus détaillée et plus précise des changements détectés.
        </p>
      </article>

      <div class="interests-grid">
        <!-- 1 -->
        <div class="interest-item">
          <i class="fas fa-image"></i>
          <h3>Prétraitement &amp; Alignement</h3>
          <p>
            Contrôle approfondi de la correspondance spatiale entre les différentes 
            prises de vue pour assurer une base de données homogène.
          </p>
        </div>

        <!-- 2 -->
        <div class="interest-item">
          <i class="fas fa-brain"></i>
          <h3>Extraction d’Embeddings</h3>
          <p>
            Utilisation de modèles de fondation pour créer des représentations locales 
            robustes, essentielles à l’analyse fine des variations dans le paysage.
          </p>
        </div>

        <!-- 3 -->
        <div class="interest-item">
          <i class="fas fa-sliders-h"></i>
          <h3>Mesures de Similarité</h3>
          <p>
            Emploi de distances cosinus et euclidiennes pour quantifier la progression 
            ou la régression des zones potentiellement concernées par des anomalies.
          </p>
        </div>

        <!-- 4 -->
        <div class="interest-item">
          <i class="fas fa-layer-group"></i>
          <h3>Masques de Changement</h3>
          <p>
            Création de masques, bâtis sur la segmentation sémantique, pour mieux 
            illustrer et mesurer les changements au niveau local.
          </p>
        </div>

        <!-- 5 -->
        <div class="interest-item">
          <i class="fas fa-project-diagram"></i>
          <h3>Clustering &amp; Hiérarchisation</h3>
          <p>
            Mise en place d’un mécanisme de regroupement (avec Bertopic, par exemple) 
            pour distinguer et prioriser les différentes formes d’anomalies environnementales.
          </p>
        </div>

        <!-- 6 -->
        <div class="interest-item">
          <i class="fas fa-chart-pie"></i>
          <h3>Analyse Statistique</h3>
          <p>
            Outils de visualisation et mesures quantitatives permettant de suivre 
            l’évolution de l’occupation des sols et d’évaluer la qualité de l’approche choisie.
          </p>
        </div>
      </div>
    </section>

    <!-- ------------------------------------------------------------------------
         PROJET 2 : Vision par Ordinateur
    ------------------------------------------------------------------------- -->
    <section class="projects" id="computerVision">
      <h2>Projet de Vision par Ordinateur</h2>
      <p class="ss-titre">Projet académique - Computer Vision</p>
      <article>
        <h3>Description du projet</h3>
        <p>
          L’objectif de ce travail était de segmenter des images satellitaires (4 bandes) 
          en sept classes distinctes. Les données provenaient du jeu de données 
          DynamicEarthNet. Nous avons comparé la performance du modèle 
          selon les continents, en l’entraînant sur quatre d’entre eux, puis 
          en évaluant les résultats sur le cinquième.
        </p>
        <h3>Approche</h3>
        <p>
          Nous avons déployé FarSeg (TorchGeo) couplé à ResNet pour obtenir une 
          segmentation fine et robuste, atteignant un mIoU supérieur à 0,7 sur un 
          ensemble d’entraînement mondial (incluant tous les continents).
        </p>
      </article>
    
      <div class="interests-grid">
        <div class="interest-item">
          <i class="fas fa-cloud"></i>
          <h3>Chargement des Données</h3>
          <p>
            Conception d’un dataloader gérant les images RGB+NIR et leurs 
            coordonnées GPS afin de diviser de façon cohérente les datasets en 
            ensembles de test ou de validation.
          </p>
        </div>
    
        <div class="interest-item">
          <i class="fas fa-robot"></i>
          <h3>Parallélisation &amp; AMP</h3>
          <p>
            Utilisation optimisée du GPU et de l’API GradScaler 
            pour réduire les temps d’entraînement, notamment via le Mixed Precision 
            afin de diminuer la consommation mémoire.
          </p>
        </div>
    
        <div class="interest-item">
          <i class="fas fa-network-wired"></i>
          <h3>FarSeg &amp; ResNet</h3>
          <p>
            Association de FarSeg et d’un ResNet pré-entraîné pour améliorer 
            la robustesse de la segmentation, quelle que soit la nature du terrain.
          </p>
        </div>

        <div class="interest-item">
          <i class="fas fa-chart-line"></i>
          <h3>Stratégie d’Entraînement</h3>
          <p>
            Mise en place d’un dégel progressif des couches (fine-tuning) 
            pour stabiliser l’apprentissage et ajuster précisément le réseau de neurones.
          </p>
        </div>
    
        <div class="interest-item">
          <i class="fas fa-chart-line"></i>
          <h3>Évaluation</h3>
          <p>
            Calcul d’indicateurs clés (IoU, précision, etc.) permettant de quantifier 
            la qualité de la segmentation sur des ensembles de données continentales variées.
          </p>
        </div>

        <div class="interest-item">
          <i class="fas fa-chart-pie"></i>
          <h3>Comparaison Multicontinent</h3>
          <p>
            Analyse comparative des performances sur chaque continent, 
            afin d’identifier les écarts entre régions géographiques et 
            d’affiner les hyperparamètres utilisés.
          </p>
        </div>
      </div>
    </section>

    <!-- ------------------------------------------------------------------------
         PROJET 3 : Séries Temporelles
    ------------------------------------------------------------------------- -->
    <section class="projects" id="serieTemp">
      <h2>Projets Séries Temporelles</h2>
      <p class="ss-titre">Projet personnel - Computer Vision</p>
      <article>
        <h3>Description du projet</h3>
        <p>
          Dans le cadre de ce projet personnel, j’ai voulu détecter et segmenter 
          les changements environnementaux en m’appuyant sur des séries temporelles 
          d’images satellites Landsat. L’idée est de suivre des phénomènes tels 
          que la déforestation, l’urbanisation ou l’impact du changement climatique. A la différence du projet AIRBUS, le but ici était d'entrainer un réseau de neurones en utilisant du transfert learning. 
        </p>
        <h3>Approche</h3>
        <p>
          Le socle repose sur un encodeur DeepLabV3 (ResNet50 pré-entraîné), 
          spécifiquement ajusté pour les images satellitaires, 
          auquel s’ajoute un réseau ConvLSTM pour modéliser la dynamique temporelle. 
          Les données semi-supervisées proviennent de frames annotées mensuellement 
          sur la base de prises de vues quotidiennes (DynamicEarthNet).
        </p>
        <p>
          <a href="https://github.com/alexandreRocchiHenry/SerieTemp_LandSat_Images" 
             target="_blank">Voir le projet</a>
        </p>
      </article>

      <div class="interests-grid">
        <div class="interest-item">
          <i class="fas fa-cloud"></i>
          <h3>Analyse de GeoTiff</h3>
          <p>
            Manipulation et exploration de données géospatiales au moyen 
            de bibliothèques comme GeoPandas et Rasterio.
          </p>
        </div>

        <div class="interest-item">
          <i class="fas fa-comments"></i>
          <h3>Alignement des images</h3>
          <p>
            Combinaison d’approches de géoréférencement et d’alignement par points-clés SIFT, 
            garantissant une superposition précise des séries temporelles.
          </p>
        </div>

        <div class="interest-item">
          <i class="fas fa-robot"></i>
          <h3>Data Augmentation Satellitaire</h3>
          <p>
            Emploi de GeoTorch et Albumentations pour enrichir la diversité 
            des clichés et améliorer la robustesse de la segmentation.
          </p>
        </div>

        <div class="interest-item">
          <i class="fas fa-network-wired"></i>
          <h3>Recherche d’Hyperparamètres</h3>
          <p>
            Pipeline articulé autour de GridSearch et Optuna 
            pour ajuster finement le modèle à des conditions de terrain variées.
          </p>
        </div>

        <div class="interest-item">
          <i class="fas fa-cogs"></i>
          <h3>Combinaison DeepLabV3 &amp; LSTM</h3>
          <p>
            Mise en place d’un modèle associant DeepLabV3 et ConvLSTM, 
            ciblé sur la détection des changements au fil du temps.
          </p>
        </div>

        <div class="interest-item">
          <i class="fas fa-chart-line"></i>
          <h3>Optical Flow</h3>
          <p>
            Préparation future d’une méthode basée sur le flux optique 
            afin de repérer les variations subtiles d’une image à l’autre.
          </p>
        </div>
      </div>
    </section>

    <!-- ------------------------------------------------------------------------
         PROJET 4 : Classification d’Images
    ------------------------------------------------------------------------- -->
    <section class="projects" id="serieTemp">
      <h2>Projet - Classification d’Images</h2>
      <p class="ss-titre">Projet personnel - Computer Vision</p>
      <article>
        <h3>Description du projet</h3>
        <p>
          Dans ce projet, l’idée était de classifier automatiquement différentes espèces 
          (ici des reptiles) à partir de photographies. J’ai donc réalisé un pipeline 
          complet, de la préparation des données (split, labellisation) jusqu’à 
          l’évaluation avec une K-fold cross-validation.
        </p>
    
        <h3>Approche</h3>
        <p>
          Nous avons opté pour un CNN de type EfficientNet (B0 ou B5), entraîné sur 
          ImageNet, couplé à un jeu d’augmentations adaptées pour améliorer la 
          robustesse de la classification.
        </p>
      </article>
    
      <div class="interests-grid">
        <!-- 1 -->
        <div class="interest-item">
          <i class="fas fa-cloud"></i>
          <h3>Pipeline de Données</h3>
          <p>
            Mise en place d’une architecture flexible pour charger et prétraiter rapidement 
            de grands volumes d’images destinées à l’entraînement.
          </p>
        </div>
    
        <!-- 2 -->
        <div class="interest-item">
          <i class="fas fa-comments"></i>
          <h3>Data Augmentation</h3>
          <p>
            Application de transformations variées (flip, rotation, zoom, etc.) afin 
            d’accroître la capacité du réseau à reconnaître chaque classe.
          </p>
        </div>
    
        <!-- 3 -->
        <div class="interest-item">
          <i class="fas fa-robot"></i>
          <h3>Approche de Transfer Learning</h3>
          <p>
            Réutilisation d’un EfficientNet pré-entraîné, suivie d’un ajustement fin 
            pour adapter le modèle aux différents reptiles ciblés.
          </p>
        </div>
    
        <!-- 4 -->
        <div class="interest-item">
          <i class="fas fa-network-wired"></i>
          <h3>Cross-validation Stratifiée</h3>
          <p>
            Emploi du K-Fold pour obtenir une évaluation plus stable et robuste, 
            pratique essentielle lorsque les données sont hétérogènes ou limitées.
          </p>
        </div>
    
        <!-- 5 -->
        <div class="interest-item">
          <i class="fas fa-cogs"></i>
          <h3>TensorFlow</h3>
          <p>
            Exploitation de la “mixed precision” pour accélérer l’entraînement et 
            prendre en charge de larges lots d’images, même sur un hardware limité.
          </p>
        </div>
    
        <!-- 6 -->
        <div class="interest-item">
          <i class="fas fa-chart-line"></i>
          <h3>Évaluation &amp; Metrics</h3>
          <p>
            Analyse fine des résultats via matrice de confusion et classification report, 
            afin de mesurer la capacité du modèle à bien distinguer les espèces.
          </p>
        </div>
      </div>
    </section>

    <!-- ------------------------------------------------------------------------
         PROJET 5 : Analyse de Manifestos & Chartes (NLP)
    ------------------------------------------------------------------------- -->
    <section class="projects" id="nlpProject">
      <h2>Projet - Analyse de Manifestos &amp; Chartes</h2>
      <p class="ss-titre">Projet académique - NLP</p>
      <article>
        <h3>Description du projet</h3>
        <p>
          L’objectif était de repérer et de regrouper les thèmes saillants 
          d’un ensemble de manifestos et de chartes portant sur l’IA et l’éthique. 
          Le but : mettre en évidence à la fois les points communs et les divergences 
          dans les discours, afin de comprendre les grandes tendances du domaine.
        </p>
    
        <h3>Approche</h3>
        <p>
          Nous avons élaboré une pipeline complète allant du prétraitement et de la vectorisation 
          des textes, jusqu’au clustering (BERTopic, LDA, GPTopic) et à l’interprétation 
          des regroupements thématiques.
        </p>
      </article>
    
      <div class="interests-grid">
        <!-- 1 -->
        <div class="interest-item">
          <i class="fas fa-code"></i>
          <h3>Pipeline de Clustering</h3>
          <p>
            Création d’un flux complet pour normaliser, vectoriser et grouper les documents, 
            permettant de faire émerger des catégories cohérentes.
          </p>
        </div>
    
        <!-- 2 -->
        <div class="interest-item">
          <i class="fas fa-brain"></i>
          <h3>Extraction de Thèmes</h3>
          <p>
            Utilisation de BERTopic pour mettre en évidence les thématiques structurantes 
            et faciliter l’identification des axes majeurs.
          </p>
        </div>
    
        <!-- 3 -->
        <div class="interest-item">
          <i class="fas fa-database"></i>
          <h3>Prétraitement des Textes</h3>
          <p>
            Nettoyage approfondi (tokenisation, élimination du bruit, etc.), 
            indispensable pour obtenir une modélisation cohérente.
          </p>
        </div>
    
        <!-- 4 -->
        <div class="interest-item">
          <i class="fas fa-chart-pie"></i>
          <h3>Évaluation des Clusters</h3>
          <p>
            Calcul de métriques telles que la cohérence sémantique, 
            pour valider la pertinence des regroupements.
          </p>
        </div>
    
        <!-- 5 -->
        <div class="interest-item">
          <i class="fas fa-network-wired"></i>
          <h3>Transférabilité des Méthodes</h3>
          <p>
            Possibilité d’étendre ces approches à d’autres secteurs, 
            notamment la vision par ordinateur pour l’analyse d’images environnementales.
          </p>
        </div>
    
        <!-- 6 -->
        <div class="interest-item">
          <i class="fas fa-eye"></i>
          <h3>Visualisation Interactive</h3>
          <p>
            Création d’outils de visualisation permettant d’explorer 
            et de communiquer clairement les regroupements thématiques au public.
          </p>
        </div>
      </div>
    </section>

    <!-- ------------------------------------------------------------------------
         PROJET 6 : Traduction Automatique de Code
    ------------------------------------------------------------------------- -->
    <section class="projects" id="projetFilRouge">
      <h2>Projet - Traduction Automatique de Code</h2>
      <p class="ss-titre">Projet de groupe - IDEMIA</p>
      <article>
        <h3>Description du projet</h3>
        <p>
          En collaboration avec IDEMIA, ce projet de groupe visait à élaborer une 
          chaîne de traitement innovante pour traduire automatiquement des codebases 
          écrites en divers langages (Python, Golang, C) vers un unique langage de 
          production (C). L’enjeu majeur : simplifier l’intégration et la maintenance 
          des démonstrateurs industriels. Pour cela, nous avons couplé prétraitement 
          avancé, modèles de langage (LLM) et tests automatisés robustes.
        </p>
    
        <h3>Approche</h3>
        <p>
          La chaîne de traitement comporte plusieurs étapes : tokenisation, extraction 
          d’arbres syntaxiques (AST), traduction via des LLM (Llama 3 ou GPT-3.5) 
          et une itération de corrections automatiques, incluant des outils de diagnostic 
          (clang-tidy, clang-format) et la génération de tests unitaires.
        </p>
      </article>
    
      <div class="interests-grid">
        <!-- 1 -->
        <div class="interest-item">
          <i class="fas fa-code"></i>
          <h3>Prétraitement Avancé</h3>
          <p>
            Intégration d’une pipeline de tokenisation et d’analyse d’AST pour 
            préparer au mieux le code à la phase de traduction automatique.
          </p>
        </div>
    
        <!-- 2 -->
        <div class="interest-item">
          <i class="fas fa-robot"></i>
          <h3>Traduction par LLM</h3>
          <p>
            Exploitation de modèles de langage de pointe pour convertir différents 
            fragments de code dans le langage cible.
          </p>
        </div>
    
        <!-- 3 -->
        <div class="interest-item">
          <i class="fas fa-check-circle"></i>
          <h3>Validation et Correction</h3>
          <p>
            Boucle de retours automatiques (clang-tidy, clang-format) et tests unitaires 
            pour assurer une qualité de code conforme aux attentes industrielles.
          </p>
        </div>
    
        <!-- 4 -->
        <div class="interest-item">
          <i class="fas fa-sitemap"></i>
          <h3>Analyse Statique</h3>
          <p>
            Repérage des dépendances et découpage en modules 
            pour garantir une traduction structurée et plus aisée à maintenir.
          </p>
        </div>
    
        <!-- 5 -->
        <div class="interest-item">
          <i class="fas fa-vial"></i>
          <h3>Cas de Tests Automatisés</h3>
          <p>
            Génération de jeux de tests pour vérifier la validité fonctionnelle 
            du code traduit, minimisant les régressions potentielles.
          </p>
        </div>
    
        <!-- 6 -->
        <div class="interest-item">
          <i class="fas fa-chart-line"></i>
          <h3>Métriques de Performance</h3>
          <p>
            Mesure de la qualité de la traduction via des scores (BLEU, compilation, etc.) 
            afin d’évaluer objectivement l’efficacité de la pipeline.
          </p>
        </div>
      </div>
    </section>

    <!-- ------------------------------------------------------------------------
         PROJET 7 : Streamlit Food.com
    ------------------------------------------------------------------------- -->
    <section class="projects" id="projetGroupe">
      <h2>Projet - Streamlit Food.com</h2>
      <p class="ss-titre">Projet académique - Big Data &amp; Analyse</p>
      <article>
        <h3>Description du projet</h3>
        <p>
          Dans ce projet mené en groupe, nous avons construit une plateforme globale 
          d’ingestion, de prétraitement et d’analyse de grandes masses de données 
          du site Food.com. L’objectif était de dégager des indicateurs clés 
          sur l’évolution du site, tout en fournissant un ensemble 
          d’outils réutilisables dans différents projets.
        </p>
        
        <h3>Approche</h3>
        <p>
          Nous avons mis en place des pipelines robustes, exploré des techniques 
          de clustering thématique (dont Bertopic) et développé des tableaux 
          de bord dynamiques pour faciliter la visualisation des résultats. 
          Ces compétences sont particulièrement utiles pour des applications 
          complexes et l'analyse de données brutes.
        </p>
      </article>
      
      <div class="interests-grid">
        <!-- 1 -->
        <div class="interest-item">
          <i class="fas fa-database"></i>
          <h3>Application Streamlit</h3>
          <p>
            Développement d’une interface interactive pour gérer la préparation des données 
            et présenter en direct les résultats de l’analyse.
          </p>
        </div>
        
        <!-- 2 -->
        <div class="interest-item">
          <i class="fas fa-project-diagram"></i>
          <h3>Clustering Thématique</h3>
          <p>
            Mise en place d’un pipeline de clustering s’appuyant sur Bertopic 
            pour extraire les tendances fortes et organiser les données.
          </p>
        </div>
        
        <!-- 3 -->
        <div class="interest-item">
          <i class="fas fa-map-marked-alt"></i>
          <h3>Pipeline CI-CD</h3>
          <p>
            Conception d’un flux de travail automatisé pour le déploiement continu 
            et l’intégration continue, facilitant la mise à jour des modèles et des données.
          </p>
        </div>
        
        <!-- 4 -->
        <div class="interest-item">
          <i class="fas fa-chart-area"></i>
          <h3>Visualisation Interactive</h3>
          <p>
            Conception de tableaux de bord graphiques et dynamiques 
            pour un suivi en temps réel des indicateurs-clés.
          </p>
        </div>
        
        <!-- 5 -->
        <div class="interest-item">
          <i class="fas fa-tachometer-alt"></i>
          <h3>Optimisation &amp; Scalabilité</h3>
          <p>
            Adaptations techniques pour accélérer les calculs et soutenir 
            des volumes de données conséquents, cruciaux dans un contexte Big Data.
          </p>
        </div>
        
        <!-- 6 -->
        <div class="interest-item">
          <i class="fas fa-users"></i>
          <h3>Collaboration Agile</h3>
          <p>
            Approche de travail itérative et multidisciplinaire, 
            favorisant la coordination efficace entre plusieurs expertises métier.
          </p>
        </div>
      </div>
    </section>
  </main>
  <footer>
    <p>&copy; 2025 Alexandre Rocchi. Tous droits réservés.</p>
  </footer>
</body>
</html>
